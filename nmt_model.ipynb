{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7a7dcb",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a83adb9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCS224N Winter 2025: Homework 3\\nnmt_model.py: NMT Model\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "CS224N Winter 2025: Homework 3\n",
    "nmt_model.py: NMT Model\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "796d964c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import sys\n",
    "from typing import List, Tuple, Dict, Set, Union\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "949ac5f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[31mDocstring:\u001b[39m\n",
       "argmax(input) -> LongTensor\n",
       "\n",
       "Returns the indices of the maximum value of all elements in the :attr:`input` tensor.\n",
       "\n",
       "This is the second value returned by :meth:`torch.max`. See its\n",
       "documentation for the exact semantics of this method.\n",
       "\n",
       ".. note:: If there are multiple maximal values then the indices of the first maximal value are returned.\n",
       "\n",
       "Args:\n",
       "    input (Tensor): the input tensor.\n",
       "\n",
       "Example::\n",
       "\n",
       "    >>> a = torch.randn(4, 4)\n",
       "    >>> a\n",
       "    tensor([[ 1.3398,  0.2663, -0.2686,  0.2450],\n",
       "            [-0.7401, -0.8805, -0.3402, -1.1936],\n",
       "            [ 0.4907, -1.3948, -1.0691, -0.3132],\n",
       "            [-1.6092,  0.5419, -0.2993,  0.3195]])\n",
       "    >>> torch.argmax(a)\n",
       "    tensor(0)\n",
       "\n",
       ".. function:: argmax(input, dim, keepdim=False) -> LongTensor\n",
       "   :noindex:\n",
       "\n",
       "Returns the indices of the maximum values of a tensor across a dimension.\n",
       "\n",
       "This is the second value returned by :meth:`torch.max`. See its\n",
       "documentation for the exact semantics of this method.\n",
       "\n",
       "Args:\n",
       "    input (Tensor): the input tensor.\n",
       "    dim (int): the dimension to reduce. If ``None``, the argmax of the flattened input is returned.\n",
       "    keepdim (bool): whether the output tensor has :attr:`dim` retained or not.\n",
       "\n",
       "Example::\n",
       "\n",
       "    >>> a = torch.randn(4, 4)\n",
       "    >>> a\n",
       "    tensor([[ 1.3398,  0.2663, -0.2686,  0.2450],\n",
       "            [-0.7401, -0.8805, -0.3402, -1.1936],\n",
       "            [ 0.4907, -1.3948, -1.0691, -0.3132],\n",
       "            [-1.6092,  0.5419, -0.2993,  0.3195]])\n",
       "    >>> torch.argmax(a, dim=1)\n",
       "    tensor([ 0,  2,  0,  1])\n",
       "\u001b[31mType:\u001b[39m      builtin_function_or_method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.argmax?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c51bca2-d1ab-4ae6-bf6f-3834d9e3783d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error importing huggingface_hub.hf_api: No module named 'urllib3.packages.six.moves'\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'urllib3.packages.six.moves'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\cs224n\\Lib\\site-packages\\transformers\\__init__.py:27\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     29\u001b[39m     OptionalDependencyNotAvailable,\n\u001b[32m     30\u001b[39m     _LazyModule,\n\u001b[32m   (...)\u001b[39m\u001b[32m     50\u001b[39m     logging,\n\u001b[32m     51\u001b[39m )\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimport_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m define_import_structure\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\cs224n\\Lib\\site-packages\\transformers\\dependency_versions_check.py:16\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdependency_versions_table\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deps\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mversions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m require_version, require_version_core\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# define which module versions we always want to check at run time\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# (usually the ones defined in `install_requires` in setup.py)\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# order specific notes:\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# - tqdm must be checked before tokenizers\u001b[39;00m\n\u001b[32m     25\u001b[39m pkgs_to_check_at_runtime = [\n\u001b[32m     26\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpython\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     27\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtqdm\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     37\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpyyaml\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     38\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\cs224n\\Lib\\site-packages\\transformers\\utils\\__init__.py:19\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#!/usr/bin/env python\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Copyright 2021 The HuggingFace Inc. team. All rights reserved.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfunctools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m lru_cache\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhuggingface_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_full_repo_name  \u001b[38;5;66;03m# for backward compatibility\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhuggingface_hub\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconstants\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HF_HUB_DISABLE_TELEMETRY \u001b[38;5;28;01mas\u001b[39;00m DISABLE_TELEMETRY  \u001b[38;5;66;03m# for backward compatibility\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpackaging\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m version\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\cs224n\\Lib\\site-packages\\huggingface_hub\\__init__.py:1028\u001b[39m, in \u001b[36m_attach.<locals>.__getattr__\u001b[39m\u001b[34m(name)\u001b[39m\n\u001b[32m   1026\u001b[39m submod_path = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpackage_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr_to_modules[name]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1027\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1028\u001b[39m     submod = importlib.import_module(submod_path)\n\u001b[32m   1029\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1030\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError importing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubmod_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\cs224n\\Lib\\importlib\\__init__.py:90\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     88\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     89\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap._gcd_import(name[level:], package, level)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\cs224n\\Lib\\site-packages\\huggingface_hub\\hf_api.py:51\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     33\u001b[39m     TYPE_CHECKING,\n\u001b[32m     34\u001b[39m     Any,\n\u001b[32m   (...)\u001b[39m\u001b[32m     47\u001b[39m     overload,\n\u001b[32m     48\u001b[39m )\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01murllib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m quote, unquote\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrequests\u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrequests\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HTTPError\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mauto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm \u001b[38;5;28;01mas\u001b[39;00m base_tqdm\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\cs224n\\Lib\\site-packages\\requests\\__init__.py:43\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#   __\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m#  /__)  _  _     _   _ _/   _\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# / (   (- (/ (/ (- _)  /  _)\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m#          /\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[33;03mRequests HTTP Library\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[33;03m~~~~~~~~~~~~~~~~~~~~~\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     40\u001b[39m \u001b[33;03m:license: Apache 2.0, see LICENSE for more details.\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01murllib3\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchardet\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\cs224n\\Lib\\site-packages\\urllib3\\__init__.py:8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m__future__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m absolute_import\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconnectionpool\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      9\u001b[39m     HTTPConnectionPool,\n\u001b[32m     10\u001b[39m     HTTPSConnectionPool,\n\u001b[32m     11\u001b[39m     connection_from_url\n\u001b[32m     12\u001b[39m )\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m exceptions\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfilepost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m encode_multipart_formdata\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\cs224n\\Lib\\site-packages\\urllib3\\connectionpool.py:11\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msocket\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m error \u001b[38;5;28;01mas\u001b[39;00m SocketError, timeout \u001b[38;5;28;01mas\u001b[39;00m SocketTimeout\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msocket\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     12\u001b[39m     ClosedPoolError,\n\u001b[32m     13\u001b[39m     ProtocolError,\n\u001b[32m     14\u001b[39m     EmptyPoolError,\n\u001b[32m     15\u001b[39m     HeaderParsingError,\n\u001b[32m     16\u001b[39m     HostChangedError,\n\u001b[32m     17\u001b[39m     LocationValueError,\n\u001b[32m     18\u001b[39m     MaxRetryError,\n\u001b[32m     19\u001b[39m     ProxyError,\n\u001b[32m     20\u001b[39m     ReadTimeoutError,\n\u001b[32m     21\u001b[39m     SSLError,\n\u001b[32m     22\u001b[39m     \u001b[38;5;167;01mTimeoutError\u001b[39;00m,\n\u001b[32m     23\u001b[39m     InsecureRequestWarning,\n\u001b[32m     24\u001b[39m     NewConnectionError,\n\u001b[32m     25\u001b[39m )\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpackages\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mssl_match_hostname\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CertificateError\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpackages\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m six\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\cs224n\\Lib\\site-packages\\urllib3\\exceptions.py:2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m__future__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m absolute_import\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpackages\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msix\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmoves\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhttp_client\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      3\u001b[39m     IncompleteRead \u001b[38;5;28;01mas\u001b[39;00m httplib_IncompleteRead\n\u001b[32m      4\u001b[39m )\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Base Exceptions\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mHTTPError\u001b[39;00m(\u001b[38;5;167;01mException\u001b[39;00m):\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'urllib3.packages.six.moves'"
     ]
    }
   ],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b6a411d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate(example):\n",
    "    return {\n",
    "        'text': \" \".join(example['text'].split()[:50]),\n",
    "        'label': example['label']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fca7a284-cd3f-423c-86be-2417bf15ffa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': \"i don't know why but i will try, how about you?\", 'label': 'positive'}, {'text': 'i tried, how about you?', 'label': 'positive'}]\n"
     ]
    }
   ],
   "source": [
    "s = [{'text':  \"i don't know why but i will try, how about you?\", 'label': \"positive\"}, {'text':  \"i tried, how about you?\", 'label': \"positive\"}]\n",
    "res = map(truncate, s)\n",
    "print(list(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fbb7ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_embeddings import ModelEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84df78ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hypothesis = namedtuple('Hypothesis', ['value', 'score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd668af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMT(nn.Module):\n",
    "    \"\"\" Simple Neural Machine Translation Model:\n",
    "        - Bidrectional LSTM Encoder\n",
    "        - Unidirection LSTM Decoder\n",
    "        - Global Attention Model (Luong, et al. 2015)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_size, hidden_size, vocab, dropout_rate=0.2):\n",
    "        \"\"\" Init NMT Model.\n",
    "\n",
    "        @param embed_size (int): Embedding size (dimensionality)\n",
    "        @param hidden_size (int): Hidden Size, the size of hidden states (dimensionality)\n",
    "        @param vocab (Vocab): Vocabulary object containing src and tgt languages\n",
    "                              See vocab.py for documentation.\n",
    "        @param dropout_rate (float): Dropout probability, for attention\n",
    "        \"\"\"\n",
    "        super(NMT, self).__init__()\n",
    "        self.model_embeddings = ModelEmbeddings(embed_size, vocab)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.vocab = vocab\n",
    "\n",
    "        # default values\n",
    "        self.post_embed_cnn = None\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        self.h_projection = None\n",
    "        self.c_projection = None\n",
    "        self.att_projection = None\n",
    "        self.combined_output_projection = None\n",
    "        self.target_vocab_projection = None\n",
    "        self.dropout = None\n",
    "        # For sanity check only, not relevant to implementation\n",
    "        self.gen_sanity_check = False\n",
    "        self.counter = 0\n",
    "\n",
    "        ### YOUR CODE HERE (~9 Lines)\n",
    "        ### TODO - Initialize the following variables IN THIS ORDER:\n",
    "        ###     self.post_embed_cnn (Conv1d layer with kernel size 2, input and output channels = embed_size,\n",
    "        ###         padding = same to preserve output shape )\n",
    "        ###     self.encoder (Bidirectional LSTM with bias)\n",
    "        ###     self.decoder (LSTM Cell with bias)\n",
    "        ###     self.h_projection (Linear Layer with no bias), called W_{h} in the PDF.\n",
    "        ###     self.c_projection (Linear Layer with no bias), called W_{c} in the PDF.\n",
    "        ###     self.att_projection (Linear Layer with no bias), called W_{attProj} in the PDF.\n",
    "        ###     self.combined_output_projection (Linear Layer with no bias), called W_{u} in the PDF.\n",
    "        ###     self.target_vocab_projection (Linear Layer with no bias), called W_{vocab} in the PDF.\n",
    "        ###     self.dropout (Dropout Layer)\n",
    "        ###\n",
    "        ### Use the following docs to properly initialize these variables:\n",
    "        ###     Conv1d:\n",
    "        ###         https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html\n",
    "        ###     LSTM:\n",
    "        ###         https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
    "        ###     LSTM Cell:\n",
    "        ###         https://pytorch.org/docs/stable/generated/torch.nn.LSTMCell.html\n",
    "        ###     Linear Layer:\n",
    "        ###         https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\n",
    "        ###     Dropout Layer:\n",
    "        ###         https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html\n",
    "        self.post_embed_cnn = nn,Conv1d(embed_size, embed_size, 2, padding=embed_size)\n",
    "        self.encoder = nn.LSTM(embed_size, hidden_size, bidirectional=True)\n",
    "        self.decoder = nn.LSTMCell(embed_size+hidden_size, hidden_size, bias=True)\n",
    "        self.h_projection = nn.Linear(2*hidden_size, hidden_size, bias=False)\n",
    "        self.c_projection = nn.Linear(2*hidden_size, hidden_size, bias=False)\n",
    "        self.att_projection = nn.Linear(2*hidden_size, hidden_size, bias=False)\n",
    "        self.combined_output_projection = nn.Linear(3*hidden_size, hidden_size, bias=False)\n",
    "        self.target_vocab_projection = nn.Linear(hidden_size, len(vocab.tgt), bias=False)\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "\n",
    "        ### END YOUR CODE\n",
    "\n",
    "    def forward(self, source: List[List[str]], target: List[List[str]]) -> torch.Tensor:\n",
    "        \"\"\" Take a mini-batch of source and target sentences, compute the log-likelihood of\n",
    "        target sentences under the language models learned by the NMT system.\n",
    "\n",
    "        @param source (List[List[str]]): list of source sentence tokens\n",
    "        @param target (List[List[str]]): list of target sentence tokens, wrapped by `<s>` and `</s>`\n",
    "\n",
    "        @returns scores (Tensor): a variable/tensor of shape (b, ) representing the\n",
    "                                    log-likelihood of generating the gold-standard target sentence for\n",
    "                                    each example in the input batch. Here b = batch size.\n",
    "        \"\"\"\n",
    "        # Compute sentence lengths\n",
    "        source_lengths = [len(s) for s in source]\n",
    "\n",
    "        # Convert list of lists into tensors\n",
    "        source_padded = self.vocab.src.to_input_tensor(source, device=self.device)  # Tensor: (src_len, b)\n",
    "        target_padded = self.vocab.tgt.to_input_tensor(target, device=self.device)  # Tensor: (tgt_len, b)\n",
    "\n",
    "        ###     Run the network forward:\n",
    "        ###     1. Apply the encoder to `source_padded` by calling `self.encode()`\n",
    "        ###     2. Generate sentence masks for `source_padded` by calling `self.generate_sent_masks()`\n",
    "        ###     3. Apply the decoder to compute combined-output by calling `self.decode()`\n",
    "        ###     4. Compute log probability distribution over the target vocabulary using the\n",
    "        ###        combined_outputs returned by the `self.decode()` function.\n",
    "\n",
    "        enc_hiddens, dec_init_state = self.encode(source_padded, source_lengths)\n",
    "        enc_masks = self.generate_sent_masks(enc_hiddens, source_lengths)\n",
    "        combined_outputs = self.decode(enc_hiddens, enc_masks, dec_init_state, target_padded)\n",
    "        P = F.log_softmax(self.target_vocab_projection(combined_outputs), dim=-1)\n",
    "\n",
    "        # Zero out, probabilities for which we have nothing in the target text\n",
    "        target_masks = (target_padded != self.vocab.tgt['<pad>']).float()\n",
    "\n",
    "        # Compute log probability of generating true target words\n",
    "        target_gold_words_log_prob = torch.gather(P, index=target_padded[1:].unsqueeze(-1), dim=-1).squeeze(\n",
    "            -1) * target_masks[1:]\n",
    "        scores = target_gold_words_log_prob.sum(dim=0)\n",
    "        return scores\n",
    "\n",
    "    def encode(self, source_padded: torch.Tensor, source_lengths: List[int], grader_params=None) -> Tuple[\n",
    "        torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        \"\"\" Apply the encoder to source sentences to obtain encoder hidden states.\n",
    "            Additionally, take the final states of the encoder and project them to obtain initial states for decoder.\n",
    "\n",
    "        @param source_padded (Tensor): Tensor of padded source sentences with shape (src_len, b), where\n",
    "                                        b = batch_size, src_len = maximum source sentence length. Note that\n",
    "                                       these have already been sorted in order of longest to shortest sentence.\n",
    "        @param source_lengths (List[int]): List of actual lengths for each of the source sentences in the batch\n",
    "        @returns enc_hiddens (Tensor): Tensor of hidden units with shape (b, src_len, h*2), where\n",
    "                                        b = batch size, src_len = maximum source sentence length, h = hidden size.\n",
    "        @returns dec_init_state (tuple(Tensor, Tensor)): Tuple of tensors representing the decoder's initial\n",
    "                                                hidden state and cell.\n",
    "        @grader_params: Ignore this parameter. It is used for grading purposes.\n",
    "        \"\"\"\n",
    "        enc_hiddens, dec_init_state = None, None\n",
    "\n",
    "        ### YOUR CODE HERE (~ 11 Lines)\n",
    "        ### TODO:\n",
    "        ###     1. Construct Tensor `X` of source sentences with shape (src_len, b, e) using the source model embeddings.\n",
    "        ###         src_len = maximum source sentence length, b = batch size, e = embedding size. Note\n",
    "        ###         that there is no initial hidden state or cell for the encoder.\n",
    "        ###     2. Apply the post_embed_cnn layer. Before feeding X into the CNN, first use torch.permute to change the\n",
    "        ###         shape of X to (b, e, src_len). After getting the output from the CNN, remember to use torch.permute\n",
    "        ###         again to revert X back to its original shape.\n",
    "        ###     3. Compute `enc_hiddens`, `last_hidden`, `last_cell` by applying the encoder to `X`.\n",
    "        ###         - Before you can apply the encoder, you need to apply the `pack_padded_sequence` function to X.\n",
    "        ###         - After you apply the encoder, you need to apply the `pad_packed_sequence` function to enc_hiddens.\n",
    "        ###         - Note that the shape of the tensor returned by the encoder is (src_len, b, h*2) and we want to\n",
    "        ###           return a tensor of shape (b, src_len, h*2) as `enc_hiddens`.\n",
    "        ###     4. Compute `dec_init_state` = (init_decoder_hidden, init_decoder_cell):\n",
    "        ###         - `init_decoder_hidden`:\n",
    "        ###             `last_hidden` is a tensor shape (2, b, h). The first dimension corresponds to forwards and backwards.\n",
    "        ###             Concatenate the forwards and backwards tensors to obtain a tensor shape (b, 2*h).\n",
    "        ###             Apply the h_projection layer to this in order to compute init_decoder_hidden.\n",
    "        ###             This is h_0^{dec} in the PDF. Here b = batch size, h = hidden size\n",
    "        ###         - `init_decoder_cell`:\n",
    "        ###             `last_cell` is a tensor shape (2, b, h). The first dimension corresponds to forwards and backwards.\n",
    "        ###             Concatenate the forwards and backwards tensors to obtain a tensor shape (b, 2*h).\n",
    "        ###             Apply the c_projection layer to this in order to compute init_decoder_cell.\n",
    "        ###             This is c_0^{dec} in the PDF. Here b = batch size, h = hidden size\n",
    "        ###\n",
    "        ### See the following docs, as you may need to use some of the following functions in your implementation:\n",
    "        ###     Pack the padded sequence X before passing to the encoder:\n",
    "        ###         https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html\n",
    "        ###     Pad the packed sequence, enc_hiddens, returned by the encoder:\n",
    "        ###         https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_packed_sequence.html\n",
    "        ###     Tensor Concatenation:\n",
    "        ###         https://pytorch.org/docs/stable/generated/torch.cat.html\n",
    "        ###     Tensor Permute:\n",
    "        ###         https://pytorch.org/docs/stable/generated/torch.permute.html\n",
    "        X = self.source(source_padded)\n",
    "        packed_sequence = nn.utils.rnn.pack_padded_sequence(torch.permute(self.post_embed_cnn(torch.permute(X, (1, 2, 0))), (2, 0, 1)))\n",
    "        enc_hiddens, (last_hidden, last_cell) = self.encoder(packed_sequence.data, source_lengths)\n",
    "        enc_hiddens = nn.utils.rnn.pad_packed_sequence(enc_hiddens, batch_first=True)\n",
    "        init_decoder_hidden = self.h_projection(torch.cat((last_hidden[0], last_hidden[1]), dim=1))\n",
    "        init_decoder_cell = self.c_projection(torch.cat((last_cell[0], last_cell[1]), dim=1))\n",
    "        dec_init_state = (init_decoder_hidden, init_decoder_cell)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        ### END YOUR CODE\n",
    "\n",
    "        return enc_hiddens, dec_init_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6788b66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "    def decode(self, enc_hiddens: torch.Tensor, enc_masks: torch.Tensor,\n",
    "               dec_init_state: Tuple[torch.Tensor, torch.Tensor], target_padded: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Compute combined output vectors for a batch.\n",
    "\n",
    "        @param enc_hiddens (Tensor): Hidden states (b, src_len, h*2), where\n",
    "                                     b = batch size, src_len = maximum source sentence length, h = hidden size.\n",
    "        @param enc_masks (Tensor): Tensor of sentence masks (b, src_len), where\n",
    "                                     b = batch size, src_len = maximum source sentence length.\n",
    "        @param dec_init_state (tuple(Tensor, Tensor)): Initial state and cell for decoder\n",
    "        @param target_padded (Tensor): Gold-standard padded target sentences (tgt_len, b), where\n",
    "                                       tgt_len = maximum target sentence length, b = batch size.\n",
    "\n",
    "        @returns combined_outputs (Tensor): combined output tensor  (tgt_len, b,  h), where\n",
    "                                        tgt_len = maximum target sentence length, b = batch_size,  h = hidden size\n",
    "        \"\"\"\n",
    "        # Chop off the <END> token for max length sentences.\n",
    "        target_padded = target_padded[:-1]\n",
    "\n",
    "        # Initialize the decoder state (hidden and cell)\n",
    "        dec_state = dec_init_state\n",
    "\n",
    "        # Initialize previous combined output vector o_{t-1} as zero\n",
    "        batch_size = enc_hiddens.size(0)\n",
    "        o_prev = torch.zeros(batch_size, self.hidden_size, device=self.device)\n",
    "\n",
    "        # Initialize a list we will use to collect the combined output o_t on each step\n",
    "        combined_outputs = []\n",
    "\n",
    "        ### YOUR CODE HERE (~9 Lines)\n",
    "        ### TODO:\n",
    "        ###     1. Apply the attention projection layer to `enc_hiddens` to obtain `enc_hiddens_proj`,\n",
    "        ###         which should be shape (b, src_len, h),\n",
    "        ###         where b = batch size, src_len = maximum source length, h = hidden size.\n",
    "        ###         This is applying W_{attProj} to h^enc, as described in the PDF.\n",
    "        ###     2. Construct tensor `Y` of target sentences with shape (tgt_len, b, e) using the target model embeddings.\n",
    "        ###         where tgt_len = maximum target sentence length, b = batch size, e = embedding size.\n",
    "        ###     3. Use the torch.split function to iterate over the time dimension of Y.\n",
    "        ###         Within the loop, this will give you Y_t of shape (1, b, e) where b = batch size, e = embedding size.\n",
    "        ###             - Squeeze Y_t into a tensor of dimension (b, e).\n",
    "        ###             - Construct Ybar_t by concatenating Y_t with o_prev on their last dimension\n",
    "        ###             - Use the step function to compute the the Decoder's next (cell, state) values\n",
    "        ###               as well as the new combined output o_t.\n",
    "        ###             - Append o_t to combined_outputs\n",
    "        ###             - Update o_prev to the new o_t.\n",
    "        ###     4. Use torch.stack to convert combined_outputs from a list length tgt_len of\n",
    "        ###         tensors shape (b, h), to a single tensor shape (tgt_len, b, h)\n",
    "        ###         where tgt_len = maximum target sentence length, b = batch size, h = hidden size.\n",
    "        ###\n",
    "        ### Note:\n",
    "        ###    - When using the squeeze() function make sure to specify the dimension you want to squeeze\n",
    "        ###      over. Otherwise, you will remove the batch dimension accidentally, if batch_size = 1.\n",
    "        ###\n",
    "        ### You may find some of these functions useful:\n",
    "        ###     Zeros Tensor:\n",
    "        ###         https://pytorch.org/docs/stable/generated/torch.zeros.html\n",
    "        ###     Tensor Splitting (iteration):\n",
    "        ###         https://pytorch.org/docs/stable/generated/torch.split.html\n",
    "        ###     Tensor Dimension Squeezing:\n",
    "        ###         https://pytorch.org/docs/stable/generated/torch.squeeze.html\n",
    "        ###     Tensor Concatenation:\n",
    "        ###         https://pytorch.org/docs/stable/generated/torch.cat.html\n",
    "        ###     Tensor Stacking:\n",
    "        ###         https://pytorch.org/docs/stable/generated/torch.stack.html\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ### END YOUR CODE\n",
    "\n",
    "        return combined_outputs\n",
    "\n",
    "    def step(self, Ybar_t: torch.Tensor,\n",
    "             dec_state: Tuple[torch.Tensor, torch.Tensor],\n",
    "             enc_hiddens: torch.Tensor,\n",
    "             enc_hiddens_proj: torch.Tensor,\n",
    "             enc_masks: torch.Tensor) -> Tuple[Tuple, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\" Compute one forward step of the LSTM decoder, including the attention computation.\n",
    "\n",
    "        @param Ybar_t (Tensor): Concatenated Tensor of [Y_t o_prev], with shape (b, e + h). The input for the decoder,\n",
    "                                where b = batch size, e = embedding size, h = hidden size.\n",
    "        @param dec_state (tuple(Tensor, Tensor)): Tuple of tensors both with shape (b, h), where b = batch size, h = hidden size.\n",
    "                First tensor is decoder's prev hidden state, second tensor is decoder's prev cell.\n",
    "        @param enc_hiddens (Tensor): Encoder hidden states Tensor, with shape (b, src_len, h * 2), where b = batch size,\n",
    "                                    src_len = maximum source length, h = hidden size.\n",
    "        @param enc_hiddens_proj (Tensor): Encoder hidden states Tensor, projected from (h * 2) to h. Tensor is with shape (b, src_len, h),\n",
    "                                    where b = batch size, src_len = maximum source length, h = hidden size.\n",
    "        @param enc_masks (Tensor): Tensor of sentence masks shape (b, src_len),\n",
    "                                    where b = batch size, src_len is maximum source length.\n",
    "\n",
    "        @returns dec_state (tuple (Tensor, Tensor)): Tuple of tensors both shape (b, h), where b = batch size, h = hidden size.\n",
    "                First tensor is decoder's new hidden state, second tensor is decoder's new cell.\n",
    "        @returns combined_output (Tensor): Combined output Tensor at timestep t, shape (b, h), where b = batch size, h = hidden size.\n",
    "        @returns e_t (Tensor): Tensor of shape (b, src_len). It is attention scores distribution.\n",
    "                                Note: You will not use this outside of this function.\n",
    "                                      We are simply returning this value so that we can sanity check\n",
    "                                      your implementation.\n",
    "        \"\"\"\n",
    "\n",
    "        combined_output = None\n",
    "\n",
    "        ### YOUR CODE HERE (~3 Lines)\n",
    "        ### TODO:\n",
    "        ###     1. Apply the decoder to `Ybar_t` and `dec_state`to obtain the new dec_state.\n",
    "        ###     2. Split dec_state into its two parts (dec_hidden, dec_cell)\n",
    "        ###     3. Compute the attention scores e_t, a Tensor shape (b, src_len).\n",
    "        ###        Note: b = batch_size, src_len = maximum source length, h = hidden size.\n",
    "        ###\n",
    "        ###       Hints:\n",
    "        ###         - dec_hidden is shape (b, h) and corresponds to h^dec_t in the PDF (batched)\n",
    "        ###         - enc_hiddens_proj is shape (b, src_len, h) and corresponds to W_{attProj} h^enc (batched).\n",
    "        ###         - Use batched matrix multiplication (torch.bmm) to compute e_t (be careful about the input/ output shapes!)\n",
    "        ###         - To get the tensors into the right shapes for bmm, you will need to do some squeezing and unsqueezing.\n",
    "        ###         - When using the squeeze() function make sure to specify the dimension you want to squeeze\n",
    "        ###             over. Otherwise, you will remove the batch dimension accidentally, if batch_size = 1.\n",
    "        ###\n",
    "        ### Use the following docs to implement this functionality:\n",
    "        ###     Batch Multiplication:\n",
    "        ###         https://pytorch.org/docs/stable/generated/torch.bmm.html\n",
    "        ###     Tensor Unsqueeze:\n",
    "        ###         https://pytorch.org/docs/stable/generated/torch.unsqueeze.html\n",
    "        ###     Tensor Squeeze:\n",
    "        ###         https://pytorch.org/docs/stable/generated/torch.squeeze.html\n",
    "\n",
    "\n",
    "        ### END YOUR CODE\n",
    "\n",
    "        # Set e_t to -inf where enc_masks has 1\n",
    "        if enc_masks is not None:\n",
    "            e_t.data.masked_fill_(enc_masks.bool(), -float('inf'))\n",
    "\n",
    "        ### YOUR CODE HERE (~6 Lines)\n",
    "        ### TODO:\n",
    "        ###     1. Apply softmax to e_t to yield alpha_t\n",
    "        ###     2. Use batched matrix multiplication between alpha_t and enc_hiddens to obtain the\n",
    "        ###         attention output vector, a_t.\n",
    "        ###           - alpha_t is shape (b, src_len)\n",
    "        ###           - enc_hiddens is shape (b, src_len, 2h)\n",
    "        ###           - a_t should be shape (b, 2h)\n",
    "        ###           - You will need to do some squeezing and unsqueezing.\n",
    "        ###     Note: b = batch size, src_len = maximum source length, h = hidden size.\n",
    "        ###\n",
    "        ###     3. Concatenate dec_hidden with a_t to compute tensor U_t\n",
    "        ###     4. Apply the combined output projection layer to U_t to compute tensor V_t\n",
    "        ###     5. Compute tensor O_t by first applying the Tanh function and then the dropout layer.\n",
    "        ###\n",
    "        ### Use the following docs to implement this functionality:\n",
    "        ###     Softmax:\n",
    "        ###         https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html\n",
    "        ###     Batch Multiplication:\n",
    "        ###         https://pytorch.org/docs/stable/generated/torch.bmm.html\n",
    "        ###     Tensor View:\n",
    "        ###         https://pytorch.org/docs/stable/generated/torch.Tensor.view.html\n",
    "        ###     Tensor Concatenation:\n",
    "        ###         https://pytorch.org/docs/stable/generated/torch.cat.html\n",
    "        ###     Tanh:\n",
    "        ###         https://pytorch.org/docs/stable/generated/torch.tanh.html\n",
    "\n",
    "\n",
    "        ### END YOUR CODE\n",
    "\n",
    "        combined_output = O_t\n",
    "        return dec_state, combined_output, e_t\n",
    "\n",
    "    def generate_sent_masks(self, enc_hiddens: torch.Tensor, source_lengths: List[int]) -> torch.Tensor:\n",
    "        \"\"\" Generate sentence masks for encoder hidden states.\n",
    "\n",
    "        @param enc_hiddens (Tensor): encodings of shape (b, src_len, 2*h), where b = batch size,\n",
    "                                     src_len = max source length, h = hidden size.\n",
    "        @param source_lengths (List[int]): List of actual lengths for each of the sentences in the batch.\n",
    "\n",
    "        @returns enc_masks (Tensor): Tensor of sentence masks of shape (b, src_len),\n",
    "                                    where src_len = max source length, h = hidden size.\n",
    "        \"\"\"\n",
    "        enc_masks = torch.zeros(enc_hiddens.size(0), enc_hiddens.size(1), dtype=torch.float)\n",
    "        for e_id, src_len in enumerate(source_lengths):\n",
    "            enc_masks[e_id, src_len:] = 1\n",
    "        return enc_masks.to(self.device)\n",
    "\n",
    "    def beam_search(self, src_sent: List[str], beam_size: int = 5, max_decoding_time_step: int = 70) -> List[\n",
    "        Hypothesis]:\n",
    "        \"\"\" Given a single source sentence, perform beam search, yielding translations in the target language.\n",
    "        @param src_sent (List[str]): a single source sentence (words)\n",
    "        @param beam_size (int): beam size\n",
    "        @param max_decoding_time_step (int): maximum number of time steps to unroll the decoding RNN\n",
    "        @returns hypotheses (List[Hypothesis]): a list of hypothesis, each hypothesis has two fields:\n",
    "                value: List[str]: the decoded target sentence, represented as a list of words\n",
    "                score: float: the log-likelihood of the target sentence\n",
    "        \"\"\"\n",
    "        src_sents_var = self.vocab.src.to_input_tensor([src_sent], self.device)\n",
    "\n",
    "        src_encodings, dec_init_vec = self.encode(src_sents_var, [len(src_sent)])\n",
    "        src_encodings_att_linear = self.att_projection(src_encodings)\n",
    "\n",
    "        h_tm1 = dec_init_vec\n",
    "        att_tm1 = torch.zeros(1, self.hidden_size, device=self.device)\n",
    "\n",
    "        eos_id = self.vocab.tgt['</s>']\n",
    "\n",
    "        hypotheses = [['<s>']]\n",
    "        hyp_scores = torch.zeros(len(hypotheses), dtype=torch.float, device=self.device)\n",
    "        completed_hypotheses = []\n",
    "\n",
    "        t = 0\n",
    "        while len(completed_hypotheses) < beam_size and t < max_decoding_time_step:\n",
    "            t += 1\n",
    "            hyp_num = len(hypotheses)\n",
    "\n",
    "            exp_src_encodings = src_encodings.expand(hyp_num,\n",
    "                                                     src_encodings.size(1),\n",
    "                                                     src_encodings.size(2))\n",
    "\n",
    "            exp_src_encodings_att_linear = src_encodings_att_linear.expand(hyp_num,\n",
    "                                                                           src_encodings_att_linear.size(1),\n",
    "                                                                           src_encodings_att_linear.size(2))\n",
    "\n",
    "            y_tm1 = torch.tensor([self.vocab.tgt[hyp[-1]] for hyp in hypotheses], dtype=torch.long, device=self.device)\n",
    "            y_t_embed = self.model_embeddings.target(y_tm1)\n",
    "\n",
    "            x = torch.cat([y_t_embed, att_tm1], dim=-1)\n",
    "\n",
    "            (h_t, cell_t), att_t, _ = self.step(x, h_tm1,\n",
    "                                                exp_src_encodings, exp_src_encodings_att_linear, enc_masks=None)\n",
    "\n",
    "            # log probabilities over target words\n",
    "            log_p_t = F.log_softmax(self.target_vocab_projection(att_t), dim=-1)\n",
    "\n",
    "            live_hyp_num = beam_size - len(completed_hypotheses)\n",
    "            contiuating_hyp_scores = (hyp_scores.unsqueeze(1).expand_as(log_p_t) + log_p_t).view(-1)\n",
    "            top_cand_hyp_scores, top_cand_hyp_pos = torch.topk(contiuating_hyp_scores, k=live_hyp_num)\n",
    "\n",
    "            prev_hyp_ids = torch.div(top_cand_hyp_pos, len(self.vocab.tgt), rounding_mode='floor')\n",
    "            hyp_word_ids = top_cand_hyp_pos % len(self.vocab.tgt)\n",
    "\n",
    "            new_hypotheses = []\n",
    "            live_hyp_ids = []\n",
    "            new_hyp_scores = []\n",
    "\n",
    "            for prev_hyp_id, hyp_word_id, cand_new_hyp_score in zip(prev_hyp_ids, hyp_word_ids, top_cand_hyp_scores):\n",
    "                prev_hyp_id = prev_hyp_id.item()\n",
    "                hyp_word_id = hyp_word_id.item()\n",
    "                cand_new_hyp_score = cand_new_hyp_score.item()\n",
    "\n",
    "                hyp_word = self.vocab.tgt.id2word[hyp_word_id]\n",
    "                new_hyp_sent = hypotheses[prev_hyp_id] + [hyp_word]\n",
    "                if hyp_word == '</s>':\n",
    "                    completed_hypotheses.append(Hypothesis(value=new_hyp_sent[1:-1],\n",
    "                                                           score=cand_new_hyp_score))\n",
    "                else:\n",
    "                    new_hypotheses.append(new_hyp_sent)\n",
    "                    live_hyp_ids.append(prev_hyp_id)\n",
    "                    new_hyp_scores.append(cand_new_hyp_score)\n",
    "\n",
    "            if len(completed_hypotheses) == beam_size:\n",
    "                break\n",
    "\n",
    "            live_hyp_ids = torch.tensor(live_hyp_ids, dtype=torch.long, device=self.device)\n",
    "            h_tm1 = (h_t[live_hyp_ids], cell_t[live_hyp_ids])\n",
    "            att_tm1 = att_t[live_hyp_ids]\n",
    "\n",
    "            hypotheses = new_hypotheses\n",
    "            hyp_scores = torch.tensor(new_hyp_scores, dtype=torch.float, device=self.device)\n",
    "\n",
    "        if len(completed_hypotheses) == 0:\n",
    "            completed_hypotheses.append(Hypothesis(value=hypotheses[0][1:],\n",
    "                                                   score=hyp_scores[0].item()))\n",
    "\n",
    "        completed_hypotheses.sort(key=lambda hyp: hyp.score, reverse=True)\n",
    "\n",
    "        return completed_hypotheses\n",
    "\n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        \"\"\" Determine which device to place the Tensors upon, CPU or GPU.\n",
    "        \"\"\"\n",
    "        return self.model_embeddings.source.weight.device\n",
    "\n",
    "    @staticmethod\n",
    "    def load(model_path: str):\n",
    "        \"\"\" Load the model from a file.\n",
    "        @param model_path (str): path to model\n",
    "        \"\"\"\n",
    "        params = torch.load(model_path, map_location=lambda storage, loc: storage)\n",
    "        args = params['args']\n",
    "        model = NMT(vocab=params['vocab'], **args)\n",
    "        model.load_state_dict(params['state_dict'])\n",
    "\n",
    "        return model\n",
    "\n",
    "    def save(self, path: str):\n",
    "        \"\"\" Save the odel to a file.\n",
    "        @param path (str): path to the model\n",
    "        \"\"\"\n",
    "        print('save model parameters to [%s]' % path, file=sys.stderr)\n",
    "\n",
    "        params = {\n",
    "            'args': dict(embed_size=self.model_embeddings.embed_size, hidden_size=self.hidden_size,\n",
    "                         dropout_rate=self.dropout_rate),\n",
    "            'vocab': self.vocab,\n",
    "            'state_dict': self.state_dict()\n",
    "        }\n",
    "\n",
    "        torch.save(params, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "625a8028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[1, 2,3], [2, 3, 1]])\n",
    "b = torch.permute(a, (1, 0))\n",
    "b.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f2b18d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Conv1d?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40e834f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e03ef540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.zeros(2, 3, 5)\n",
    "b = torch.cat((a[0], a[1]), dim=1)\n",
    "b.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38963d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "executable": "/usr/bin/env python3",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "cs224n",
   "language": "python",
   "name": "cs224n"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
